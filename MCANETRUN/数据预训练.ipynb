{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1>药物</h1>",
   "id": "25b650406dd71a5a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T03:06:59.165630Z",
     "start_time": "2025-03-14T03:06:59.082749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('Davis.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "data = []\n",
    "for line in lines:\n",
    "    parts = line.strip().split(' ', 4)\n",
    "    if len(parts) == 5:\n",
    "        compound_id, protein_name, smiles, rest = parts[0], parts[1], parts[2], parts[3] + ' ' + parts[4]\n",
    "        sequence, label = rest.rsplit(' ', 1)\n",
    "        data.append({\n",
    "            'compound_id': compound_id,\n",
    "            'protein_name': protein_name,\n",
    "            'smiles': smiles,\n",
    "            'sequence': sequence,\n",
    "            'label': int(label)\n",
    "        })"
   ],
   "id": "a5446d198203de7d",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T03:06:59.168722Z",
     "start_time": "2025-03-14T03:06:59.166750Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(data))",
   "id": "7c002172c8423dda",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T03:06:59.249887Z",
     "start_time": "2025-03-14T03:06:59.169708Z"
    }
   },
   "cell_type": "code",
   "source": "data",
   "id": "50891c7b5009b127",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T03:06:59.257896Z",
     "start_time": "2025-03-14T03:06:59.251404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "smiles = set()\n",
    "for smile in data:\n",
    "    smiles.add(smile['smiles'])"
   ],
   "id": "7175db8ccb2f3715",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T03:06:59.262681Z",
     "start_time": "2025-03-14T03:06:59.259866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for smile in smiles:\n",
    "    print(smile)"
   ],
   "id": "e622d74a2a2e822b",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T03:06:59.266559Z",
     "start_time": "2025-03-14T03:06:59.263850Z"
    }
   },
   "cell_type": "code",
   "source": "smiles = list(smiles)",
   "id": "60666a901eb1b84d",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T03:06:59.270136Z",
     "start_time": "2025-03-14T03:06:59.267778Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(smiles))",
   "id": "6a09c9cd9302bad7",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T03:07:00.349037Z",
     "start_time": "2025-03-14T03:06:59.271274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cuda')"
   ],
   "id": "99879c3314c56e15",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T03:07:10.201950Z",
     "start_time": "2025-03-14T03:07:00.349637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from transformers import AutoTokenizer, RobertaModel\n",
    "# \n",
    "#  = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-10M-MTR\")\n",
    "#  = RobertaModel.from_pretrained(\"DeepChem/ChemBERTa-10M-MTR\").to(device)\n",
    "# model.eval()\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\n",
    "model = AutoModel.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\").to(device)\n",
    "model.eval()"
   ],
   "id": "b5e42c66f9432f63",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "tokenizer里包括里input_ids和attention_mask",
   "id": "dc03d1848b166113"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T03:07:10.217928Z",
     "start_time": "2025-03-14T03:07:10.204778Z"
    }
   },
   "cell_type": "code",
   "source": "input = tokenizer(smiles,padding=True, truncation=True,max_length=512, return_tensors=\"pt\").to(device)",
   "id": "3cd8d02b90ee46b1",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T03:07:10.351074Z",
     "start_time": "2025-03-14T03:07:10.219217Z"
    }
   },
   "cell_type": "code",
   "source": "input",
   "id": "679be83aafa6c659",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T03:07:10.599184Z",
     "start_time": "2025-03-14T03:07:10.351787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    output = model(**input)"
   ],
   "id": "dd7e5cbd32f9b40e",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T03:07:10.846541Z",
     "start_time": "2025-03-14T03:07:10.600796Z"
    }
   },
   "cell_type": "code",
   "source": "output.pooler_output",
   "id": "71f6e65e1fe7d09c",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T03:07:10.849882Z",
     "start_time": "2025-03-14T03:07:10.847385Z"
    }
   },
   "cell_type": "code",
   "source": "(output.pooler_output).shape",
   "id": "f37dd50b240bc461",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T03:07:10.853620Z",
     "start_time": "2025-03-14T03:07:10.850439Z"
    }
   },
   "cell_type": "code",
   "source": "output.last_hidden_state.shape",
   "id": "3104063abcdf00de",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T03:07:10.873595Z",
     "start_time": "2025-03-14T03:07:10.854258Z"
    }
   },
   "cell_type": "code",
   "source": "output.last_hidden_state.mean(dim=1)",
   "id": "79026b04ff7a7d8e",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T03:07:10.877243Z",
     "start_time": "2025-03-14T03:07:10.874294Z"
    }
   },
   "cell_type": "code",
   "source": "output.keys()",
   "id": "50e3efa2dde1326f",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "这个要保存为cpu不然保存不了，gpu的话怕适配不了所以要先保存为cpu",
   "id": "252b321598693c09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T03:07:10.908835Z",
     "start_time": "2025-03-14T03:07:10.877980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "features = output.last_hidden_state.cpu()\n",
    "torch.save(features, 'ligands_davis.pt')"
   ],
   "id": "cbad1e1097b8335b",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T03:07:10.917980Z",
     "start_time": "2025-03-14T03:07:10.910664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "features = torch.load('ligands_davis.pt')\n",
    "print(features.shape)"
   ],
   "id": "868f8c2e371ce3a5",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1>蛋白质</h1>",
   "id": "ab9a75f2db7b1deb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T03:07:11.026989Z",
     "start_time": "2025-03-14T03:07:10.922694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('Davis.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "data = []\n",
    "for line in lines:\n",
    "    parts = line.strip().split(' ', 4)\n",
    "    if len(parts) == 5:\n",
    "        compound_id, protein_name, smiles, rest = parts[0], parts[1], parts[2], parts[3] + ' ' + parts[4]\n",
    "        sequence, label = rest.rsplit(' ', 1)\n",
    "        data.append({\n",
    "            'compound_id': compound_id,\n",
    "            'protein_name': protein_name,\n",
    "            'smiles': smiles,\n",
    "            'sequence': sequence,\n",
    "            'label': int(label)\n",
    "        })\n",
    "\n",
    "proteins = set()\n",
    "for protein in data:\n",
    "    proteins.add(protein['sequence'])"
   ],
   "id": "d8a6c7c4aa52ad61",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T03:07:11.031932Z",
     "start_time": "2025-03-14T03:07:11.027730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "proteins =list(proteins)\n",
    "for protein in proteins:\n",
    "    print(protein)"
   ],
   "id": "bc548435328f008f",
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T03:07:11.783999Z",
     "start_time": "2025-03-14T03:07:11.032683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cuda')\n",
    "import re\n",
    "\n",
    "# Load model directly\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "\n",
    "local_path_model = \"prot_t5_xl_uniref50/\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(local_path_model)\n",
    "model = T5EncoderModel.from_pretrained(local_path_model).to(device)\n",
    "model.eval()\n",
    "\n",
    "sequence_examples = proteins\n",
    "# this will replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids\n",
    "sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n",
    "\n",
    "# tokenize sequences and pad up to the longest sequence in the batch\n",
    "ids = tokenizer.batch_encode_plus(sequence_examples, add_special_tokens=True, padding=\"longest\")\n",
    "input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "\n",
    "# generate embeddings\n",
    "with torch.no_grad():\n",
    "    embedding_repr = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "\n",
    "# extract embeddings for the first ([0,:]) sequence in the batch while removing padded & special tokens ([0,:7]) \n",
    "emb_0 = embedding_repr.last_hidden_state[0,:7] # shape (7 x 1024)\n",
    "print(f\"Shape of per-residue embedding of first sequences: {emb_0.shape}\")\n",
    "# do the same for the second ([1,:]) sequence in the batch while taking into account different sequence lengths ([1,:8])\n",
    "emb_1 = embedding_repr.last_hidden_state[1,:8] # shape (8 x 1024)\n",
    "\n",
    "# if you want to derive a single representation (per-protein embedding) for the whole protein\n",
    "emb_0_per_protein = emb_0.mean(dim=0) # shape (1024)\n",
    "\n",
    "print(f\"Shape of per-protein embedding of first sequences: {emb_0_per_protein.shape}\")"
   ],
   "id": "6a0bc544862f46f2",
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "with open('Davis.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "data = []\n",
    "for line in lines:\n",
    "    parts = line.strip().split(' ', 4)\n",
    "    if len(parts) == 5:\n",
    "        compound_id, protein_name, smiles, rest = parts[0], parts[1], parts[2], parts[3] + ' ' + parts[4]\n",
    "        sequence, label = rest.rsplit(' ', 1)\n",
    "        data.append({\n",
    "            'compound_id': compound_id,\n",
    "            'protein_name': protein_name,\n",
    "            'smiles': smiles,\n",
    "            'sequence': sequence,\n",
    "            'label': int(label)\n",
    "        })\n",
    "\n",
    "proteins = set()\n",
    "for protein in data:\n",
    "    proteins.add(protein['sequence'])\n",
    "\n",
    "proteins = list(proteins)\n",
    "print(len(proteins))"
   ],
   "id": "4451d5429c90af68",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "#local_path = \"/Volumes/PASSPORT/FinalYearProject/ChemBERTa-77M-MLM\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\n",
    "model = AutoModel.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\").to(device)\n",
    "model.eval()"
   ],
   "id": "804b1de0e156b245",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "｜分子相识性",
   "id": "3cb325d7c59a11a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "import torch\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "local_path = \"/Volumes/PASSPORT/FinalYearProject/ChemBERTa-77M-MLM\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_path)\n",
    "model = AutoModel.from_pretrained(local_path).to(device)\n",
    "\n",
    "smiles_1 = [\"CCO\"]\n",
    "smiles_2 = [\"CCN\"]\n",
    "\n",
    "tokens_1 = tokenizer(smiles_1, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "tokens_2 = tokenizer(smiles_2, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    emb_1 = model(**tokens_1).pooler_output  # 形状: (1, 768)\n",
    "    emb_2 = model(**tokens_2).pooler_output\n",
    "\n",
    "similarity = cosine_similarity(emb_1, emb_2)\n",
    "print(f\"SMILES 相似度: {similarity.item()}\")\n"
   ],
   "id": "48d176dd72253221",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c14d43ef4119ba53",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
