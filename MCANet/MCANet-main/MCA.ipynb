{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:57:38.187952Z",
     "start_time": "2024-12-15T02:57:38.185527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ],
   "id": "30a10424b5ff3668",
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-15T02:54:06.710631Z",
     "start_time": "2024-12-15T02:54:06.705775Z"
    }
   },
   "source": [
    "CHARISOSMISET = {\"#\": 29, \"%\": 30, \")\": 31, \"(\": 1, \"+\": 32, \"-\": 33, \"/\": 34, \".\": 2,\n",
    "                 \"1\": 35, \"0\": 3, \"3\": 36, \"2\": 4, \"5\": 37, \"4\": 5, \"7\": 38, \"6\": 6,\n",
    "                 \"9\": 39, \"8\": 7, \"=\": 40, \"A\": 41, \"@\": 8, \"C\": 42, \"B\": 9, \"E\": 43,\n",
    "                 \"D\": 10, \"G\": 44, \"F\": 11, \"I\": 45, \"H\": 12, \"K\": 46, \"M\": 47, \"L\": 13,\n",
    "                 \"O\": 48, \"N\": 14, \"P\": 15, \"S\": 49, \"R\": 16, \"U\": 50, \"T\": 17, \"W\": 51,\n",
    "                 \"V\": 18, \"Y\": 52, \"[\": 53, \"Z\": 19, \"]\": 54, \"\\\\\": 20, \"a\": 55, \"c\": 56,\n",
    "                 \"b\": 21, \"e\": 57, \"d\": 22, \"g\": 58, \"f\": 23, \"i\": 59, \"h\": 24, \"m\": 60,\n",
    "                 \"l\": 25, \"o\": 61, \"n\": 26, \"s\": 62, \"r\": 27, \"u\": 63, \"t\": 28, \"y\": 64}\n",
    "\n",
    "CHARISOSMILEN = 64\n",
    "\n",
    "CHARPROTSET = {\"A\": 1, \"C\": 2, \"B\": 3, \"E\": 4, \"D\": 5, \"G\": 6,\n",
    "               \"F\": 7, \"I\": 8, \"H\": 9, \"K\": 10, \"M\": 11, \"L\": 12,\n",
    "               \"O\": 13, \"N\": 14, \"Q\": 15, \"P\": 16, \"S\": 17, \"R\": 18,\n",
    "               \"U\": 19, \"T\": 20, \"W\": 21, \"V\": 22, \"Y\": 23, \"X\": 24, \"Z\": 25}\n",
    "\n",
    "CHARPROTLEN = 25"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:54:06.714224Z",
     "start_time": "2024-12-15T02:54:06.711458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def label_smiles(line, smi_ch_ind, MAX_SMI_LEN=100):\n",
    "    \"\"\"\n",
    "    将 SMILES 字符串转换为数值向量表示。\n",
    "\n",
    "    参数:\n",
    "        line (str): SMILES 字符串\n",
    "        smi_ch_ind (dict): SMILES 字符到索引的映射字典\n",
    "        MAX_SMI_LEN (int): 最大 SMILES 长度 (默认 100)\n",
    "\n",
    "    返回:\n",
    "        np.ndarray: 长度为 MAX_SMI_LEN 的数值向量\n",
    "    \"\"\"\n",
    "    X = np.zeros(MAX_SMI_LEN, dtype=np.int64())\n",
    "    for i, ch in enumerate(line[:MAX_SMI_LEN]):\n",
    "        X[i] = smi_ch_ind[ch]\n",
    "    return X"
   ],
   "id": "4fa8cbf39670d6a3",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:54:06.717347Z",
     "start_time": "2024-12-15T02:54:06.714801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def label_sequence(line, smi_ch_ind, MAX_SEQ_LEN=1000):\n",
    "    \"\"\"\n",
    "    将蛋白质序列字符串转换为数值向量表示。\n",
    "\n",
    "    参数:\n",
    "        line (str): 蛋白质序列字符串\n",
    "        smi_ch_ind (dict): 序列字符到索引的映射字典\n",
    "        MAX_SEQ_LEN (int): 最大序列长度 (默认 1000)\n",
    "\n",
    "    返回:\n",
    "        np.ndarray: 长度为 MAX_SEQ_LEN 的数值向量\n",
    "    \"\"\"\n",
    "    X = np.zeros(MAX_SEQ_LEN, np.int64())\n",
    "    for i, ch in enumerate(line[:MAX_SEQ_LEN]):\n",
    "        X[i] = smi_ch_ind[ch]\n",
    "    return X"
   ],
   "id": "ee6e8c21a5a849e6",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:54:06.723444Z",
     "start_time": "2024-12-15T02:54:06.718917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "\n",
    "    def __init__(self, savepath=None, patience=7, verbose=False, delta=0, num_n_fold=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = -np.inf\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        self.num_n_fold = num_n_fold\n",
    "        self.savepath = savepath\n",
    "\n",
    "    def __call__(self, score, model, num_epoch):\n",
    "\n",
    "        if self.best_score == -np.inf:\n",
    "            self.save_checkpoint(score, model, num_epoch)\n",
    "            self.best_score = score\n",
    "\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(\n",
    "                f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.save_checkpoint(score, model, num_epoch)\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, score, model, num_epoch):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f'Have a new best checkpoint: ({self.best_score:.6f} --> {score:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.savepath +\n",
    "                   '/valid_best_checkpoint.pth')"
   ],
   "id": "6072a87544ba096a",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:54:06.725619Z",
     "start_time": "2024-12-15T02:54:06.724087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PolyLoss(nn.Module):\n",
    "    def __init__(self, weight_loss, DEVICE, epsilon=1.0):\n",
    "        super(PolyLoss, self).__init__()\n",
    "        self.CELoss = nn.CrossEntropyLoss(weight=weight_loss, reduction='none')\n",
    "        self.epsilon = epsilon\n",
    "        self.DEVICE = DEVICE\n",
    "\n",
    "    def forward(self, predicted, labels):\n",
    "        one_hot = torch.zeros((16, 2), device=self.DEVICE).scatter_(\n",
    "            1, torch.unsqueeze(labels, dim=-1), 1)\n",
    "        pt = torch.sum(one_hot * F.softmax(predicted, dim=1), dim=-1)\n",
    "        ce = self.CELoss(predicted, labels)\n",
    "        poly1 = ce + self.epsilon * (1-pt)\n",
    "        return torch.mean(poly1)\n",
    "\n",
    "\n",
    "class CELoss(nn.Module):\n",
    "    def __init__(self, weight_CE, DEVICE):\n",
    "        super(CELoss, self).__init__()\n",
    "        self.CELoss = nn.CrossEntropyLoss(weight=weight_CE)\n",
    "        self.DEVICE = DEVICE\n",
    "\n",
    "    def forward(self, predicted, labels):\n",
    "        return self.CELoss(predicted, labels)"
   ],
   "id": "a033097f529854b8",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:58:05.620113Z",
     "start_time": "2024-12-15T02:58:05.600197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MCANet(nn.Module):\n",
    "    def __init__(self, hp,\n",
    "                 protein_MAX_LENGH=1000,\n",
    "                 drug_MAX_LENGH=100):\n",
    "        super(MCANet, self).__init__()\n",
    "        self.dim = hp.char_dim\n",
    "        self.conv = hp.conv\n",
    "        self.drug_MAX_LENGTH = drug_MAX_LENGH\n",
    "        self.drug_kernel = hp.drug_kernel\n",
    "        self.protein_MAX_LENGTH = protein_MAX_LENGH\n",
    "        self.protein_kernel = hp.protein_kernel\n",
    "        self.drug_vocab_size = 65\n",
    "        self.protein_vocab_size = 26\n",
    "        self.attention_dim = hp.conv * 4\n",
    "        self.drug_dim_afterCNNs = self.drug_MAX_LENGTH - \\\n",
    "            self.drug_kernel[0] - self.drug_kernel[1] - self.drug_kernel[2] + 3\n",
    "        self.protein_dim_afterCNNs = self.protein_MAX_LENGTH - \\\n",
    "            self.protein_kernel[0] - self.protein_kernel[1] - \\\n",
    "            self.protein_kernel[2] + 3\n",
    "        self.drug_attention_head = 5\n",
    "        self.protein_attention_head = 7\n",
    "        self.mix_attention_head = 5\n",
    "\n",
    "        self.drug_embed = nn.Embedding(\n",
    "            self.drug_vocab_size, self.dim, padding_idx=0)\n",
    "        self.protein_embed = nn.Embedding(\n",
    "            self.protein_vocab_size, self.dim, padding_idx=0)\n",
    "\n",
    "        self.Drug_CNNs = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.dim, out_channels=self.conv,\n",
    "                      kernel_size=self.drug_kernel[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=self.conv, out_channels=self.conv * 2,\n",
    "                      kernel_size=self.drug_kernel[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=self.conv*2, out_channels=self.conv * 4,\n",
    "                      kernel_size=self.drug_kernel[2]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.Drug_max_pool = nn.MaxPool1d(self.drug_dim_afterCNNs)\n",
    "        self.Protein_CNNs = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.dim, out_channels=self.conv,\n",
    "                      kernel_size=self.protein_kernel[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=self.conv, out_channels=self.conv * 2,\n",
    "                      kernel_size=self.protein_kernel[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=self.conv * 2, out_channels=self.conv * 4,\n",
    "                      kernel_size=self.protein_kernel[2]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.Protein_max_pool = nn.MaxPool1d(self.protein_dim_afterCNNs)\n",
    "\n",
    "        self.mix_attention_layer = nn.MultiheadAttention(\n",
    "            self.attention_dim, self.mix_attention_head)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.fc1 = nn.Linear(self.conv*8, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.out = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, drug, protein):\n",
    "        # [B, F_O] -> [B, F_O, D_E]\n",
    "        # [B, T_O] -> [B, T_O, D_E]\n",
    "        drugembed = self.drug_embed(drug)\n",
    "        proteinembed = self.protein_embed(protein)\n",
    "        # [B, F_O, D_E] -> [B, D_E, F_O]\n",
    "        # [B, T_O, D_E] -> [B, D_E, T_O]\n",
    "        drugembed = drugembed.permute(0, 2, 1)\n",
    "        proteinembed = proteinembed.permute(0, 2, 1)\n",
    "\n",
    "        # [B, D_E, F_O] -> [B, D_C, F_C]\n",
    "        # [B, D_E, T_O] -> [B, D_C, T_C]\n",
    "        drugConv = self.Drug_CNNs(drugembed)\n",
    "        proteinConv = self.Protein_CNNs(proteinembed)\n",
    "\n",
    "        # [B, D_C, F_C] -> [F_C, B, D_C]\n",
    "        # [B, D_C, T_C] -> [T_C, B, D_C]\n",
    "        drug_QKV = drugConv.permute(2, 0, 1)\n",
    "        protein_QKV = proteinConv.permute(2, 0, 1)\n",
    "\n",
    "        # cross Attention\n",
    "        # [F_C, B, D_C] -> [F_C, B, D_C]\n",
    "        # [T_C, B, D_C] -> [T_C, B, D_C]\n",
    "        drug_att, _ = self.mix_attention_layer(drug_QKV, protein_QKV, protein_QKV)\n",
    "        protein_att, _ = self.mix_attention_layer(protein_QKV, drug_QKV, drug_QKV)\n",
    "\n",
    "        # [F_C, B, D_C] -> [B, D_C, F_C]\n",
    "        # [T_C, B, D_C] -> [B, D_C, T_C]\n",
    "        drug_att = drug_att.permute(1, 2, 0)\n",
    "        protein_att = protein_att.permute(1, 2, 0)\n",
    "\n",
    "        drugConv = drugConv * 0.5 + drug_att * 0.5\n",
    "        proteinConv = proteinConv * 0.5 + protein_att * 0.5\n",
    "\n",
    "        drugConv = self.Drug_max_pool(drugConv).squeeze(2)\n",
    "        proteinConv = self.Protein_max_pool(proteinConv).squeeze(2)\n",
    "\n",
    "        pair = torch.cat([drugConv, proteinConv], dim=1)\n",
    "        pair = self.dropout1(pair)\n",
    "        fully1 = self.leaky_relu(self.fc1(pair))\n",
    "        fully1 = self.dropout2(fully1)\n",
    "        fully2 = self.leaky_relu(self.fc2(fully1))\n",
    "        fully2 = self.dropout3(fully2)\n",
    "        fully3 = self.leaky_relu(self.fc3(fully2))\n",
    "        predict = self.out(fully3)\n",
    "        return predict\n",
    "\n",
    "\n",
    "class onlyPolyLoss(nn.Module):\n",
    "    def __init__(self, hp,\n",
    "                 protein_MAX_LENGH=1000,\n",
    "                 drug_MAX_LENGH=100):\n",
    "        super(onlyPolyLoss, self).__init__()\n",
    "        self.dim = hp.char_dim\n",
    "        self.conv = hp.conv\n",
    "        self.drug_MAX_LENGH = drug_MAX_LENGH\n",
    "        self.drug_kernel = hp.drug_kernel\n",
    "        self.protein_MAX_LENGH = protein_MAX_LENGH\n",
    "        self.protein_kernel = hp.protein_kernel\n",
    "        self.drug_vocab_size = 65\n",
    "        self.protein_vocab_size = 26\n",
    "        self.attention_dim = hp.conv * 4\n",
    "        self.durg_dim_afterCNNs = self.drug_MAX_LENGH - \\\n",
    "            self.drug_kernel[0] - self.drug_kernel[1] - self.drug_kernel[2] + 3\n",
    "        self.protein_dim_afterCNNs = self.protein_MAX_LENGH - \\\n",
    "            self.protein_kernel[0] - self.protein_kernel[1] - \\\n",
    "            self.protein_kernel[2] + 3\n",
    "\n",
    "        self.drug_embed = nn.Embedding(\n",
    "            self.drug_vocab_size, self.dim, padding_idx=0)\n",
    "        self.protein_embed = nn.Embedding(\n",
    "            self.protein_vocab_size, self.dim, padding_idx=0)\n",
    "\n",
    "        self.Drug_CNNs = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.dim, out_channels=self.conv,\n",
    "                      kernel_size=self.drug_kernel[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=self.conv, out_channels=self.conv * 2,\n",
    "                      kernel_size=self.drug_kernel[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=self.conv*2, out_channels=self.conv * 4,\n",
    "                      kernel_size=self.drug_kernel[2]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.Drug_max_pool = nn.MaxPool1d(self.durg_dim_afterCNNs)\n",
    "        self.Protein_CNNs = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.dim, out_channels=self.conv,\n",
    "                      kernel_size=self.protein_kernel[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=self.conv, out_channels=self.conv * 2,\n",
    "                      kernel_size=self.protein_kernel[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=self.conv * 2, out_channels=self.conv * 4,\n",
    "                      kernel_size=self.protein_kernel[2]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.Protein_max_pool = nn.MaxPool1d(self.protein_dim_afterCNNs)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.fc1 = nn.Linear(self.conv*8, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.out = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, drug, protein):\n",
    "        # [B, F_O] -> [B, F_O, D_E]\n",
    "        # [B, T_O] -> [B, T_O, D_E]\n",
    "        drugembed = self.drug_embed(drug)\n",
    "        proteinembed = self.protein_embed(protein)\n",
    "\n",
    "        # [B, F_O, D_E] -> [B, D_E, F_O]\n",
    "        # [B, T_O, D_E] -> [B, D_E, T_O]\n",
    "        drugembed = drugembed.permute(0, 2, 1)\n",
    "        proteinembed = proteinembed.permute(0, 2, 1)\n",
    "\n",
    "        # [B, D_E, F_O] -> [B, D_C, F_C]\n",
    "        # [B, D_E, T_O] -> [B, D_C, T_C]\n",
    "        drugConv = self.Drug_CNNs(drugembed)\n",
    "        proteinConv = self.Protein_CNNs(proteinembed)\n",
    "\n",
    "        drugConv = self.Drug_max_pool(drugConv).squeeze(2)\n",
    "        proteinConv = self.Protein_max_pool(proteinConv).squeeze(2)\n",
    "\n",
    "        pair = torch.cat([drugConv, proteinConv], dim=1)\n",
    "        pair = self.dropout1(pair)\n",
    "        fully1 = self.leaky_relu(self.fc1(pair))\n",
    "        fully1 = self.dropout2(fully1)\n",
    "        fully2 = self.leaky_relu(self.fc2(fully1))\n",
    "        fully2 = self.dropout3(fully2)\n",
    "        fully3 = self.leaky_relu(self.fc3(fully2))\n",
    "        predict = self.out(fully3)\n",
    "        return predict"
   ],
   "id": "2d12adf31cd34024",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T02:58:34.059276Z",
     "start_time": "2024-12-15T02:58:34.056342Z"
    }
   },
   "cell_type": "code",
   "source": "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "id": "147ed081bcfdc037",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T03:05:15.429487Z",
     "start_time": "2024-12-15T03:05:15.424658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class hyperparameter():\n",
    "    def __init__(self):\n",
    "        self.Learning_rate = 0.001\n",
    "        self.Epoch = 200\n",
    "        self.Batch_size = 16\n",
    "        self.Patience = 50\n",
    "        #学习率衰减的间隔轮数\n",
    "        self.decay_interval = 10\n",
    "        #学习率衰退\n",
    "        self.lr_decay = 0.5\n",
    "        # 权重衰减（用于正则化）\n",
    "        self.weight_decay = 1e-4\n",
    "        self.embed_dim = 64\n",
    "        self.protein_kernel = [4, 8, 12]\n",
    "        self.drug_kernel = [4, 6, 8]\n",
    "        self.conv = 40\n",
    "        self.char_dim = 64\n",
    "        self.loss_epsilon = 1"
   ],
   "id": "43bf42263d219692",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T03:58:02.982824Z",
     "start_time": "2024-12-15T03:58:02.978645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def shuffle_dataset(dataset, seed):\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(dataset)\n",
    "    return dataset"
   ],
   "id": "be1133bc1d338962",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T03:59:58.335641Z",
     "start_time": "2024-12-15T03:59:58.332309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_kfold_data(i, datasets, k=5):\n",
    "    fold_size = len(datasets) // k\n",
    "    val_start = i * fold_size\n",
    "    if i != k - 1 and i != 0:\n",
    "        val_end = (i + 1) * fold_size\n",
    "        validset = datasets[val_start:val_end]\n",
    "        trainset = datasets[0:val_start] + datasets[val_end:]\n",
    "    elif i == 0:\n",
    "        val_end = fold_size\n",
    "        validset = datasets[val_start:val_end]\n",
    "        trainset = datasets[val_end:]\n",
    "    else:\n",
    "        validset = datasets[val_start:] \n",
    "        trainset = datasets[0:val_start]\n",
    "\n",
    "    return trainset, validset"
   ],
   "id": "251bcc621cedf7ec",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T04:00:40.137698Z",
     "start_time": "2024-12-15T04:00:40.135129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.pairs[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)"
   ],
   "id": "c8062257663bfd0f",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T04:04:17.279245Z",
     "start_time": "2024-12-15T04:04:17.275071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collate_fn(batch_data):\n",
    "    N = len(batch_data)\n",
    "    drug_ids, protein_ids = [], []\n",
    "    compound_max = 100\n",
    "    protein_max = 1000\n",
    "    compound_new = torch.zeros((N, compound_max), dtype=torch.long)\n",
    "    protein_new = torch.zeros((N, protein_max), dtype=torch.long)\n",
    "    labels_new = torch.zeros(N, dtype=torch.long)\n",
    "    for i, pair in enumerate(batch_data):\n",
    "        pair = pair.strip().split()\n",
    "        drug_id, protein_id, compoundstr, proteinstr, label = pair[-5], pair[-4], pair[-3], pair[-2], pair[-1]\n",
    "        drug_ids.append(drug_id)\n",
    "        protein_ids.append(protein_id)\n",
    "        compoundint = torch.from_numpy(label_smiles(\n",
    "            compoundstr, CHARISOSMISET, compound_max))\n",
    "        compound_new[i] = compoundint\n",
    "        proteinint = torch.from_numpy(label_sequence(\n",
    "            proteinstr, CHARPROTSET, protein_max))\n",
    "        protein_new[i] = proteinint\n",
    "        label = float(label)\n",
    "        #labels_new[i] = np.int(label)\n",
    "        labels_new[i] = int(label)\n",
    "    return (compound_new, protein_new, labels_new)"
   ],
   "id": "56ad7338007c9437",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T04:54:42.633193Z",
     "start_time": "2024-12-15T04:54:42.619928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_precess(MODEL, pbar, LOSS, DEVICE, FOLD_NUM):\n",
    "    if isinstance(MODEL, list):\n",
    "        for item in MODEL:\n",
    "            #在测试的时候使用eval()函数\n",
    "            item.eval()\n",
    "    else:\n",
    "        MODEL.eval()\n",
    "    test_losses = []\n",
    "    Y, P, S = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i, data in pbar:\n",
    "            '''data preparation '''\n",
    "            compounds, proteins, labels = data\n",
    "            compounds = compounds.to(DEVICE)\n",
    "            proteins = proteins.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            if isinstance(MODEL, list):\n",
    "                predicted_scores = torch.zeros(2).to(DEVICE)\n",
    "                for i in range(len(MODEL)):\n",
    "                    predicted_scores = predicted_scores + \\\n",
    "                        MODEL[i](compounds, proteins)\n",
    "                predicted_scores = predicted_scores / FOLD_NUM\n",
    "            else:\n",
    "                predicted_scores = MODEL(compounds, proteins)\n",
    "            loss = LOSS(predicted_scores, labels)\n",
    "            correct_labels = labels.to('cpu').data.numpy()\n",
    "            predicted_scores = F.softmax(\n",
    "                predicted_scores, 1).to('cpu').data.numpy()\n",
    "            predicted_labels = np.argmax(predicted_scores, axis=1)\n",
    "            predicted_scores = predicted_scores[:, 1]\n",
    "\n",
    "            Y.extend(correct_labels)\n",
    "            P.extend(predicted_labels)\n",
    "            S.extend(predicted_scores)\n",
    "            test_losses.append(loss.item())\n",
    "    Precision = precision_score(Y, P)\n",
    "    Recall = recall_score(Y, P)\n",
    "    AUC = roc_auc_score(Y, S)\n",
    "    tpr, fpr, _ = precision_recall_curve(Y, S)\n",
    "    PRC = auc(fpr, tpr)\n",
    "    Accuracy = accuracy_score(Y, P)\n",
    "    test_loss = np.average(test_losses)\n",
    "    return Y, P, test_loss, Accuracy, Precision, Recall, AUC, PRC\n",
    "\n",
    "def test_model(MODEL, dataset_loader, save_path, DATASET, LOSS, DEVICE, dataset_class=\"Train\", save=True, FOLD_NUM=1):\n",
    "    test_pbar = tqdm(\n",
    "        enumerate(\n",
    "            BackgroundGenerator(dataset_loader)),\n",
    "        total=len(dataset_loader))\n",
    "    T, P, loss_test, Accuracy_test, Precision_test, Recall_test, AUC_test, PRC_test = test_precess(\n",
    "        MODEL, test_pbar, LOSS, DEVICE, FOLD_NUM)\n",
    "    if save:\n",
    "        if FOLD_NUM == 1:\n",
    "            filepath = save_path + \\\n",
    "                \"/{}_{}_prediction.txt\".format(DATASET, dataset_class)\n",
    "        else:\n",
    "            filepath = save_path + \\\n",
    "                \"/{}_{}_ensemble_prediction.txt\".format(DATASET, dataset_class)\n",
    "        with open(filepath, 'a') as f:\n",
    "            for i in range(len(T)):\n",
    "                f.write(str(T[i]) + \" \" + str(P[i]) + '\\n')\n",
    "    results = '{}: Loss:{:.5f};Accuracy:{:.5f};Precision:{:.5f};Recall:{:.5f};AUC:{:.5f};PRC:{:.5f}.' \\\n",
    "        .format(dataset_class, loss_test, Accuracy_test, Precision_test, Recall_test, AUC_test, PRC_test)\n",
    "    print(results)\n",
    "    return results, Accuracy_test, Precision_test, Recall_test, AUC_test, PRC_test"
   ],
   "id": "557e493aaa1abec5",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T04:58:28.661743Z",
     "start_time": "2024-12-15T04:58:28.657350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def show_result(DATASET, Accuracy_List, Precision_List, Recall_List, AUC_List, AUPR_List, Ensemble=False):\n",
    "    Accuracy_mean, Accuracy_var = np.mean(Accuracy_List), np.var(Accuracy_List)\n",
    "    Precision_mean, Precision_var = np.mean(\n",
    "        Precision_List), np.var(Precision_List)\n",
    "    Recall_mean, Recall_var = np.mean(Recall_List), np.var(Recall_List)\n",
    "    AUC_mean, AUC_var = np.mean(AUC_List), np.var(AUC_List)\n",
    "    PRC_mean, PRC_var = np.mean(AUPR_List), np.var(AUPR_List)\n",
    "\n",
    "    if Ensemble == False:\n",
    "        print(\"The model's results:\")\n",
    "        filepath = \"./{}/results.txt\".format(DATASET)\n",
    "    else:\n",
    "        print(\"The ensemble model's results:\")\n",
    "        filepath = \"./{}/ensemble_results.txt\".format(DATASET)\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write('Accuracy(std):{:.4f}({:.4f})'.format(\n",
    "            Accuracy_mean, Accuracy_var) + '\\n')\n",
    "        f.write('Precision(std):{:.4f}({:.4f})'.format(\n",
    "            Precision_mean, Precision_var) + '\\n')\n",
    "        f.write('Recall(std):{:.4f}({:.4f})'.format(\n",
    "            Recall_mean, Recall_var) + '\\n')\n",
    "        f.write('AUC(std):{:.4f}({:.4f})'.format(AUC_mean, AUC_var) + '\\n')\n",
    "        f.write('PRC(std):{:.4f}({:.4f})'.format(PRC_mean, PRC_var) + '\\n')\n",
    "    print('Accuracy(std):{:.4f}({:.4f})'.format(Accuracy_mean, Accuracy_var))\n",
    "    print('Precision(std):{:.4f}({:.4f})'.format(\n",
    "        Precision_mean, Precision_var))\n",
    "    print('Recall(std):{:.4f}({:.4f})'.format(Recall_mean, Recall_var))\n",
    "    print('AUC(std):{:.4f}({:.4f})'.format(AUC_mean, AUC_var))\n",
    "    print('PRC(std):{:.4f}({:.4f})'.format(PRC_mean, PRC_var))"
   ],
   "id": "acc74d2874b1c8d3",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T04:58:30.422382Z",
     "start_time": "2024-12-15T04:58:29.837662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from prefetch_generator import BackgroundGenerator\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "\n",
    "def run_model(SEED, DATASET, MODEL, K_Fold, LOSS):\n",
    "    '''set random seed'''\n",
    "    random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    \n",
    "    '''init hyperparameters'''\n",
    "    hp = hyperparameter()\n",
    "    \n",
    "    '''load data'''\n",
    "    dir_input = ('./DataSets/{}.txt'.format(DATASET))\n",
    "    with open(dir_input, \"r\") as f:\n",
    "        data_list = f.read().strip().split('\\n')\n",
    "    \n",
    "    '''set loss function weight'''\n",
    "    if DATASET == \"Davis\":\n",
    "        weight_loss = torch.FloatTensor([0.3, 0.7]).to(DEVICE)\n",
    "    elif DATASET == \"KIBA\":\n",
    "        weight_loss = torch.FloatTensor([0.2, 0.8]).to(DEVICE)\n",
    "    else:\n",
    "        weight_loss = None\n",
    "    \n",
    "    '''shuffle data'''\n",
    "    data_list = shuffle_dataset(data_list, SEED)\n",
    "\n",
    "    '''split dataset to train&validation set and test set'''\n",
    "    split_pos = len(data_list) - int(len(data_list) * 0.2)\n",
    "    train_data_list = data_list[0:split_pos]\n",
    "    test_data_list = data_list[split_pos:-1]\n",
    "    print('Number of Train&Val set: {}'.format(len(train_data_list)))\n",
    "    print('Number of Test set: {}'.format(len(test_data_list)))\n",
    "\n",
    "    '''metrics'''\n",
    "    Accuracy_List_stable, AUC_List_stable, AUPR_List_stable, Recall_List_stable, Precision_List_stable = [], [], [], [], []\n",
    "\n",
    "    for i_fold in range(K_Fold):\n",
    "        print('*' * 25, 'No.', i_fold + 1, '-fold', '*' * 25)\n",
    "    \n",
    "    train_dataset, valid_dataset = get_kfold_data(\n",
    "            i_fold, train_data_list, k=K_Fold)\n",
    "    train_dataset = CustomDataSet(train_dataset)\n",
    "    valid_dataset = CustomDataSet(valid_dataset)\n",
    "    test_dataset = CustomDataSet(test_data_list)\n",
    "    train_size = len(train_dataset)\n",
    "\n",
    "    train_dataset_loader = DataLoader(train_dataset, batch_size=hp.Batch_size, shuffle=True, num_workers=0,\n",
    "                                      collate_fn=collate_fn, drop_last=True)\n",
    "    valid_dataset_loader = DataLoader(valid_dataset, batch_size=hp.Batch_size, shuffle=False, num_workers=0,\n",
    "                                      collate_fn=collate_fn, drop_last=True)\n",
    "    test_dataset_loader = DataLoader(test_dataset, batch_size=hp.Batch_size, shuffle=False, num_workers=0,\n",
    "                                     collate_fn=collate_fn, drop_last=True)\n",
    "    \"\"\" create model\"\"\"\n",
    "    model = MODEL(hp).to(DEVICE)\n",
    "    \n",
    "    \"\"\"Initialize weights\"\"\"\n",
    "    weight_p, bias_p = [], []\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    for name, p in model.named_parameters():\n",
    "        if 'bias' in name:\n",
    "            bias_p += [p]\n",
    "        else:\n",
    "            weight_p += [p]\n",
    "    \n",
    "    \"\"\"create optimizer and scheduler\"\"\"\n",
    "    optimizer = optim.AdamW(\n",
    "        [{'params': weight_p, 'weight_decay': hp.weight_decay}, {'params': bias_p, 'weight_decay': 0}], lr=hp.Learning_rate)\n",
    "\n",
    "    #学习率调度器\n",
    "    scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=hp.Learning_rate, max_lr=hp.Learning_rate*10, cycle_momentum=False,step_size_up=train_size // hp.Batch_size)\n",
    "    \n",
    "    if LOSS == 'PolyLoss':\n",
    "        Loss = PolyLoss(weight_loss=weight_loss,\n",
    "                    DEVICE=DEVICE, epsilon=hp.loss_epsilon)\n",
    "    else:\n",
    "        Loss = CELoss(weight_CE=weight_loss, DEVICE=DEVICE)\n",
    "        \n",
    "    \"\"\"Output files\"\"\"\n",
    "    save_path = \"./\" + DATASET + \"/{}\".format(i_fold+1)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    file_results = save_path + '/' + 'The_results_of_whole_dataset.txt'\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        savepath=save_path, patience=hp.Patience, verbose=True, delta=0)\n",
    "\n",
    "    for epoch in range(1, hp.Epoch + 1):\n",
    "        if early_stopping.early_stop == True:\n",
    "            break\n",
    "        train_pbar = tqdm(\n",
    "                enumerate(BackgroundGenerator(train_dataset_loader)),\n",
    "                total=len(train_dataset_loader))\n",
    "        \"\"\"train\"\"\"\n",
    "        train_losses_in_epoch = []\n",
    "        model.train()\n",
    "        \n",
    "        for train_i, train_data in train_pbar:\n",
    "            train_compounds, train_proteins, train_labels = train_data\n",
    "            train_compounds = train_compounds.to(DEVICE)\n",
    "            train_proteins = train_proteins.to(DEVICE)\n",
    "            train_labels = train_labels.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            #前向传播\n",
    "            predicted_interaction = model(train_compounds,train_proteins)\n",
    "            train_loss = Loss(predicted_interaction, train_labels)\n",
    "            train_losses_in_epoch.append(train_loss.item())\n",
    "            #反向传播\n",
    "            train_loss.backward()\n",
    "            #更新参数\n",
    "            optimizer.step()\n",
    "            #更新学习率\n",
    "            scheduler.step()\n",
    "        train_loss_a_epoch = np.average(train_losses_in_epoch)  # 一次epoch的平均训练loss\n",
    "        \n",
    "        \"\"\"valid\"\"\"\n",
    "        valid_pbar = tqdm(enumerate(BackgroundGenerator(valid_dataset_loader)),total=len(valid_dataset_loader))\n",
    "        \n",
    "        valid_losses_in_epoch = []\n",
    "        model.eval()\n",
    "        \n",
    "        #标签，预测结果，预测分数\n",
    "        Y, P, S = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for valid_i, valid_data in valid_pbar:\n",
    "\n",
    "                valid_compounds, valid_proteins,valid_labels = valid_data\n",
    "\n",
    "                valid_compounds = valid_compounds.to(DEVICE)\n",
    "                valid_proteins = valid_proteins.to(DEVICE)\n",
    "                valid_labels = valid_labels.to(DEVICE)\n",
    "\n",
    "                #预测结果\n",
    "                valid_scores = model(valid_compounds, valid_proteins)\n",
    "                #计算验证损失\n",
    "                valid_loss = Loss(valid_scores, valid_labels)\n",
    "                valid_losses_in_epoch.append(valid_loss.item())\n",
    "                #计算验证指标\n",
    "                #valid_labels = valid_labels.to('cpu').data.numpy()\n",
    "                valid_labels=valid_labels.detach().cpu().numpy()\n",
    "                #Softmax函数\n",
    "                valid_scores = F.softmax(valid_scores, 1).to('cpu').data.numpy()\n",
    "                #预测类别\n",
    "                valid_predictions = np.argmax(valid_scores, axis=1)\n",
    "                #提取正类概率\n",
    "                valid_scores = valid_scores[:, 1]\n",
    "\n",
    "                Y.extend(valid_labels)\n",
    "                P.extend(valid_predictions)\n",
    "                S.extend(valid_scores)\n",
    "\n",
    "        \"\"\"\n",
    "        1. 准确率 输入: Y:真实标签,P:预测标签\n",
    "        2. 召回率 输入: Y:真实标签,P:预测标签\n",
    "        3. 准确率 输入: Y:真实标签,P:预测标签\n",
    "        4. ROC曲线下的面积 输入: Y:真实标签,S:预测分数\n",
    "        5. PRC (Precision-Recall Curve 下的面积) 输入: Y:真实标签,S:预测分数\n",
    "        \"\"\"\n",
    "        Precision_dev = precision_score(Y, P)\n",
    "        Reacll_dev = recall_score(Y, P)\n",
    "        Accuracy_dev = accuracy_score(Y, P)\n",
    "        AUC_dev = roc_auc_score(Y, S)\n",
    "        tpr, fpr, _ = precision_recall_curve(Y, S)\n",
    "        PRC_dev = auc(fpr, tpr)\n",
    "        \n",
    "        #验证集平均损失\n",
    "        valid_loss_a_epoch = np.average(valid_losses_in_epoch)\n",
    "\n",
    "        epoch_len = len(str(hp.Epoch))\n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{hp.Epoch:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss_a_epoch:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss_a_epoch:.5f} ' +\n",
    "                     f'valid_AUC: {AUC_dev:.5f} ' +\n",
    "                     f'valid_PRC: {PRC_dev:.5f} ' +\n",
    "                     f'valid_Accuracy: {Accuracy_dev:.5f} ' +\n",
    "                     f'valid_Precision: {Precision_dev:.5f} ' +\n",
    "                     f'valid_Reacll: {Reacll_dev:.5f} ')\n",
    "        print(print_msg)\n",
    "        \n",
    "        '''save checkpoint and make decision when early stop'''\n",
    "        early_stopping(Accuracy_dev, model, epoch)\n",
    "        \n",
    "        '''load best checkpoint'''\n",
    "    model.load_state_dict(torch.load(early_stopping.savepath + '/valid_best_checkpoint.pth'))\n",
    "    \n",
    "    '''test model'''\n",
    "    trainset_test_stable_results, _, _, _, _, _ = test_model(model, train_dataset_loader, save_path, DATASET, Loss, DEVICE, dataset_class=\"Train\", FOLD_NUM=1)\n",
    "    validset_test_stable_results, _, _, _, _, _ = test_model(model, valid_dataset_loader, save_path, DATASET, Loss, DEVICE,dataset_class=\"Valid\", FOLD_NUM=1)\n",
    "    testset_test_stable_results, Accuracy_test, Precision_test,Recall_test, AUC_test, PRC_test = test_model(model, test_dataset_loader,save_path, DATASET, Loss, DEVICE, dataset_class=\"Test\", FOLD_NUM=1)\n",
    "\n",
    "    AUC_List_stable.append(AUC_test)\n",
    "    Accuracy_List_stable.append(Accuracy_test)\n",
    "    AUPR_List_stable.append(PRC_test)\n",
    "    Recall_List_stable.append(Recall_test)\n",
    "    Precision_List_stable.append(Precision_test)\n",
    "    \n",
    "    with open(save_path + '/' + \"The_results_of_whole_dataset.txt\", 'a') as f:\n",
    "        f.write(\"Test the stable model\" + '\\n')\n",
    "        f.write(trainset_test_stable_results + '\\n')\n",
    "        f.write(validset_test_stable_results + '\\n')\n",
    "        f.write(testset_test_stable_results + '\\n')\n",
    "    show_result(DATASET, Accuracy_List_stable, Precision_List_stable,\n",
    "                Recall_List_stable, AUC_List_stable, AUPR_List_stable, Ensemble=False)"
   ],
   "id": "c6cf2d106737418a",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "84f03756d54ac806",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
